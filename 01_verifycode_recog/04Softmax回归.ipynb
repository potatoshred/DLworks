{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤔如何加载数据？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels.txt\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "tags = []\n",
    "data = []\n",
    "for l in raw:\n",
    "    tags.append(int(l[0]))  # 每行的第一个字符是标签\n",
    "    d = l[1:-1]  # 去掉标签和换行符\n",
    "    d = map(float, tuple(d))  # 将字符串转换为tuple，数字转换为float，方便后续转为tensor\n",
    "    # tuple相对于list更省内存，因为tuple是不可变的，对象所含method更少\n",
    "    data.append(tuple(d))\n",
    "\n",
    "# 将标签和数据转为tensor，方便后续切分训练集和测试集\n",
    "data = torch.tensor(data)\n",
    "tags = torch.tensor(tags)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_test_ratio = 0.8\n",
    "train_size = int(train_test_ratio * len(data))\n",
    "test_size = len(data) - train_size\n",
    "data_train = data[:train_size]\n",
    "data_test = data[train_size:]\n",
    "tags_train = tags[:train_size]\n",
    "tags_test = tags[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接套用d2l网站上的代码，没有改动\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "w = torch.normal(0, 0.01, size=(\n",
    "    len(data[0]), 1), requires_grad=True, dtype=torch.float32)  # 对每个像素都有一个权重\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇方案2：softmax回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学原理：\n",
    "\n",
    "Softmax回归是一种分类算法，它可以将输入数据映射到一个固定维度的输出向量，每个元素对应于输入数据属于各个类别的概率。\n",
    "\n",
    "- Softmax回归的公式：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y} &= \\text{softmax}(Wx+b) \\\\\n",
    "\n",
    "\\text{softmax}(x_i) &= \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "其中，$W$和$b$是模型的参数，$x$是输入数据，$\\hat{y}$是模型的输出。\n",
    "\n",
    "- Softmax回归的损失函数：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L &= -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^K t_{ij}\\log\\hat{y}_{ij} \\\\\n",
    "&= -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^K t_{ij}\\log\\frac{\\exp(x_i^T w_j + b_j)}{\\sum_{k=1}^K \\exp(x_i^T w_k + b_k)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中，$t_{ij}$是样本$i$的真实标签，$K$是类别数。\n",
    "\n",
    "- Softmax回归的优化算法：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{repeat until convergence} \\\\\n",
    "\\text{for each example} \\\\\n",
    "&\\text{compute } \\hat{y}_i = \\text{softmax}(Wx_i+b) \\\\\n",
    "&\\text{compute } \\delta_i = \\hat{y}_i - t_i \\\\\n",
    "&\\text{compute } \\nabla_w L = \\frac{1}{N}\\sum_{i=1}^N \\delta_i x_i^T \\\\\n",
    "&\\text{compute } \\nabla_b L = \\frac{1}{N}\\sum_{i=1}^N \\delta_i \\\\\n",
    "&\\text{update } w = w - \\eta \\nabla_w L \\\\\n",
    "&\\text{update } b = b - \\eta \\nabla_b L\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- [Softmax的实现：](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-concise.html#subsec-softmax-implementation-revisited)\n",
    "\n",
    ">  尽管我们要计算指数函数，但我们最终在计算交叉熵损失时会取它们的对数。 通过将softmax和交叉熵结合在一\n",
    ">  起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。 如下面的等式所示，我们\n",
    ">避免计算 $\\exp(o_j  - \\max(o_k))$， 而可以直接使用$o_j - \\max(o_k)$，因为$\\log(\\exp(\\cdot))$被抵消了。\n",
    "\n",
    ">我们也希望保留传统的softmax函数，以备我们需要评估通过模型输出的概率。 但是，我们没有将softmax概率传\n",
    ">递到损失函数中， 而是在交叉熵损失函数中传递未规范化的预测，并同时计算softmax及其对数， 这是一种类似\n",
    ">“LogSumExp技巧”的聪明方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
