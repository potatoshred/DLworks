{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤”æ€è€ƒï¼šå¦‚ä½•åŠ è½½æ•°æ®ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels.txt\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "tags = []\n",
    "data = []\n",
    "for l in raw:\n",
    "    tags.append(int(l[0]))#æ¯è¡Œçš„ç¬¬ä¸€ä¸ªå­—ç¬¦æ˜¯æ ‡ç­¾\n",
    "    d = l[1:-1]#å»æ‰æ ‡ç­¾å’Œæ¢è¡Œç¬¦\n",
    "    d = map(float, tuple(d)) #å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºtupleï¼Œæ•°å­—è½¬æ¢ä¸ºfloatï¼Œæ–¹ä¾¿åç»­è½¬ä¸ºtensor\n",
    "    #tupleç›¸å¯¹äºlistæ›´çœå†…å­˜ï¼Œå› ä¸ºtupleæ˜¯ä¸å¯å˜çš„ï¼Œå¯¹è±¡æ‰€å«methodæ›´å°‘\n",
    "    data.append(tuple(d))\n",
    "\n",
    "#å°†æ ‡ç­¾å’Œæ•°æ®è½¬ä¸ºtensorï¼Œæ–¹ä¾¿åç»­åˆ‡åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "data = torch.tensor(data)\n",
    "tags = torch.tensor(tags)\n",
    "\n",
    "#åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_test_ratio = 0.8\n",
    "train_size = int(train_test_ratio * len(data))\n",
    "test_size = len(data) - train_size\n",
    "data_train = data[:train_size]\n",
    "data_test  = data[train_size:]\n",
    "tags_train = tags[:train_size]\n",
    "tags_test  = tags[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›´æ¥å¥—ç”¨d2lç½‘ç«™ä¸Šçš„ä»£ç ï¼Œæ²¡æœ‰æ”¹åŠ¨\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # è¿™äº›æ ·æœ¬æ˜¯éšæœºè¯»å–çš„ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡ºåº\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "w = torch.normal(0, 0.01, size=(len(data[0]), 1), requires_grad=True, dtype=torch.float32) #å¯¹æ¯ä¸ªåƒç´ éƒ½æœ‰ä¸€ä¸ªæƒé‡\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‡æ–¹æ¡ˆ1ï¼šLinear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒæ¨¡å‹\n",
    "def linreg(X, w, b):\n",
    "    return torch.matmul(X, w) + b\n",
    "\n",
    "# æŸå¤±å‡½æ•°\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return torch.mean((y_pred - y_true)**2)\n",
    "\n",
    "# ä¼˜åŒ–å™¨\n",
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:    \n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sgdä¼˜åŒ–å™¨æ•°å­¦åŸç†ï¼š\n",
    "- è®¡ç®—æ¢¯åº¦ï¼š$\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w},b)} \\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{w}, b)$\n",
    "- æ›´æ–°å‚æ•°ï¼š$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\cdot \\mathbf{g}$\n",
    "- $\\eta$ æ˜¯å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ›´æ–°æ­¥é•¿\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01, correct rate on training set: 1.47%\n",
      "epoch 02, correct rate on training set: 12.61%\n",
      "epoch 03, correct rate on training set: 1.17%\n",
      "epoch 04, correct rate on training set: 21.99%\n",
      "epoch 05, correct rate on training set: 21.70%\n",
      "epoch 06, correct rate on training set: 21.99%\n",
      "epoch 07, correct rate on training set: 10.26%\n",
      "epoch 08, correct rate on training set: 1.17%\n",
      "epoch 09, correct rate on training set: 10.26%\n",
      "epoch 10, correct rate on training set: 21.99%\n",
      "epoch 11, correct rate on training set: 10.26%\n",
      "epoch 12, correct rate on training set: 12.02%\n",
      "epoch 13, correct rate on training set: 10.26%\n",
      "epoch 14, correct rate on training set: 1.17%\n",
      "epoch 15, correct rate on training set: 0.00%\n",
      "epoch 16, correct rate on training set: 9.97%\n"
     ]
    }
   ],
   "source": [
    "lr = 0.02\n",
    "num_epochs = 16\n",
    "net = linreg\n",
    "loss = mse_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, data_train, tags_train):\n",
    "        l = loss(net(X, w, b), y)  # Xå’Œyçš„å°æ‰¹é‡æŸå¤±\n",
    "        # å› ä¸ºlå½¢çŠ¶æ˜¯(batch_size,1)ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚lä¸­çš„æ‰€æœ‰å…ƒç´ è¢«åŠ åˆ°ä¸€èµ·ï¼Œ\n",
    "        # å¹¶ä»¥æ­¤è®¡ç®—å…³äº[w,b]çš„æ¢¯åº¦\n",
    "        l.sum().backward() # æ±‚æ¢¯åº¦\n",
    "        sgd([w, b], lr, batch_size)  # ä½¿ç”¨å‚æ•°çš„æ¢¯åº¦æ›´æ–°å‚æ•°\n",
    "    with torch.no_grad():\n",
    "        train_l = torch.round(net(data_train, w, b)).reshape(tags_train.shape) == tags_train  # è®­ç»ƒé›†ä¸Šçš„æŸå¤±\n",
    "        print(f'epoch {epoch + 1:02d}, correct rate on training set: {train_l.sum()/len(data_train)*100:.02f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘†å¯ä»¥çœ‹åˆ°ï¼Œæ­£ç¡®ç‡ä¸Šä¸å»ï¼Œè¯´æ˜å¯èƒ½ä¸å­˜åœ¨çº¿æ€§å¯åˆ†çš„æƒ…å†µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct rate on test set: 6.98%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_l = torch.round(net(data_test, w, b)).reshape(tags_test.shape) == tags_test  # è®­ç»ƒé›†ä¸Šçš„æŸå¤±\n",
    "    print(f'correct rate on test set: {train_l.sum()/len(tags_test)*100:.02f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‡æ–¹æ¡ˆ2ï¼šsoftmaxå›å½’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1, loss: 1.636, acc: 0.974\n",
      "epoch  2, loss: 1.229, acc: 0.974\n",
      "epoch  3, loss: 0.769, acc: 0.974\n",
      "epoch  4, loss: 0.691, acc: 0.974\n",
      "epoch  5, loss: 0.443, acc: 0.974\n",
      "epoch  6, loss: 0.343, acc: 0.974\n",
      "epoch  7, loss: 0.268, acc: 0.974\n",
      "epoch  8, loss: 0.253, acc: 0.974\n",
      "epoch  9, loss: 0.204, acc: 0.974\n",
      "epoch 10, loss: 0.359, acc: 0.974\n",
      "epoch 11, loss: 0.191, acc: 0.974\n",
      "epoch 12, loss: 0.152, acc: 0.974\n",
      "epoch 13, loss: 0.307, acc: 0.974\n",
      "epoch 14, loss: 0.140, acc: 0.974\n",
      "epoch 15, loss: 0.616, acc: 0.974\n",
      "epoch 16, loss: 0.259, acc: 0.974\n",
      "epoch 17, loss: 0.411, acc: 0.974\n",
      "epoch 18, loss: 0.241, acc: 0.974\n",
      "epoch 19, loss: 0.283, acc: 0.974\n",
      "epoch 20, loss: 0.097, acc: 0.974\n",
      "epoch 21, loss: 0.267, acc: 0.974\n",
      "epoch 22, loss: 0.234, acc: 0.974\n",
      "epoch 23, loss: 0.091, acc: 0.974\n",
      "epoch 24, loss: 0.256, acc: 0.974\n",
      "epoch 25, loss: 0.226, acc: 0.974\n",
      "epoch 26, loss: 0.081, acc: 0.974\n",
      "epoch 27, loss: 0.491, acc: 0.974\n",
      "epoch 28, loss: 0.220, acc: 0.974\n",
      "epoch 29, loss: 0.078, acc: 0.974\n",
      "epoch 30, loss: 0.067, acc: 0.974\n",
      "epoch 31, loss: 0.064, acc: 0.974\n",
      "epoch 32, loss: 0.319, acc: 0.974\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "def accuracy(y_hat, y):  \n",
    "    \"\"\"è®¡ç®—é¢„æµ‹æ­£ç¡®çš„æ•°é‡\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum()/len(y))\n",
    "\n",
    "\n",
    "# å®šä¹‰æ•°æ®é›†\n",
    "batch_size = 32\n",
    "net = nn.Sequential(nn.Flatten(), nn.Linear(384, 10))\n",
    "net.apply(init_weights)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.015)\n",
    "num_epochs = 32\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼\n",
    "    for X, y in data_iter(batch_size, data_train, tags_train):\n",
    "        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "\n",
    "        # ä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°\n",
    "        trainer.zero_grad()\n",
    "        l.mean().backward()\n",
    "        trainer.step()\n",
    "\n",
    "    # è®¡ç®—è®­ç»ƒå‡†ç¡®åº¦\n",
    "    with torch.no_grad():\n",
    "        acc = accuracy(net(data_train), tags_train)\n",
    "    print(f'epoch {epoch+1:2d}, loss: {l.mean().item():.3f}, acc: {acc:.3f}')\n",
    "    # net.eval()\n",
    "    # acc = accuracy(net(data_train), tags_train)\n",
    "    # print(f'epoch {epoch+1:2d}, loss: {l.mean().item():.3f}, acc: {acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.319, acc: 0.965\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat = net(data_test)\n",
    "    y = tags_test  # æ ‡ç­¾è½¬ä¸ºLongTensorç±»å‹\n",
    "    acc = accuracy(y_hat, y)\n",
    "    print(f'loss: {l.mean().item():.3f}, acc: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‡æ–¹æ¡ˆ3ï¼šMulti-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(384, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 10))\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights)\n",
    "batch_size, lr, num_epochs = 32, 0.1, 16\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.2860, train acc 0.814, test acc 0.256\n",
      "epoch 2, loss 0.2464, train acc 1.593, test acc 0.523\n",
      "epoch 3, loss 0.1567, train acc 2.977, test acc 0.837\n",
      "epoch 4, loss 0.0746, train acc 3.663, test acc 0.837\n",
      "epoch 5, loss 0.0421, train acc 3.837, test acc 0.965\n",
      "epoch 6, loss 0.0299, train acc 3.860, test acc 0.965\n",
      "epoch 7, loss 0.0235, train acc 3.860, test acc 0.965\n",
      "epoch 8, loss 0.0210, train acc 3.860, test acc 0.965\n",
      "epoch 9, loss 0.0191, train acc 3.860, test acc 0.965\n",
      "epoch 10, loss 0.0171, train acc 3.860, test acc 0.965\n",
      "epoch 11, loss 0.0183, train acc 3.860, test acc 0.965\n",
      "epoch 12, loss 0.0158, train acc 3.860, test acc 0.965\n",
      "epoch 13, loss 0.0160, train acc 3.860, test acc 0.965\n",
      "epoch 14, loss 0.0155, train acc 3.860, test acc 0.965\n",
      "epoch 15, loss 0.0148, train acc 3.860, test acc 0.965\n",
      "epoch 16, loss 0.0155, train acc 3.860, test acc 0.965\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "    for X, y in data_iter(batch_size, data_train, tags_train):\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        l = l.mean()\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_l_sum += l.item()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        n += y.shape[0]\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter(batch_size, data_test, tags_test):\n",
    "            y_hat = net(X)\n",
    "            acc_sum += (y_hat.argmax(dim=1) == y).float().sum().item()\n",
    "            n += y.shape[0]\n",
    "    # n = test_size\n",
    "    test_acc = acc_sum / n\n",
    "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "          % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‡æ–¹æ¡ˆ4ï¼šConvolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
