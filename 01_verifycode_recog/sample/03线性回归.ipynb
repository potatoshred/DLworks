{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤔思考：如何加载数据？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels.txt\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "tags = []\n",
    "data = []\n",
    "for l in raw:\n",
    "    tags.append(int(l[0]))#每行的第一个字符是标签\n",
    "    d = l[1:-1]#去掉标签和换行符\n",
    "    d = map(float, tuple(d)) #将字符串转换为tuple，数字转换为float，方便后续转为tensor\n",
    "    #tuple相对于list更省内存，因为tuple是不可变的，对象所含method更少\n",
    "    data.append(tuple(d))\n",
    "\n",
    "#将标签和数据转为tensor，方便后续切分训练集和测试集\n",
    "data = torch.tensor(data)\n",
    "tags = torch.tensor(tags)\n",
    "\n",
    "#划分训练集和测试集\n",
    "train_test_ratio = 0.8\n",
    "train_size = int(train_test_ratio * len(data))\n",
    "test_size = len(data) - train_size\n",
    "data_train = data[:train_size]\n",
    "data_test  = data[train_size:]\n",
    "tags_train = tags[:train_size]\n",
    "tags_test  = tags[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接套用d2l网站上的代码，没有改动\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "w = torch.normal(0, 0.01, size=(len(data[0]), 1), requires_grad=True, dtype=torch.float32) #对每个像素都有一个权重\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇方案1：Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def linreg(X, w, b):\n",
    "    return torch.matmul(X, w) + b\n",
    "\n",
    "# 损失函数\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return torch.mean((y_pred - y_true)**2)\n",
    "\n",
    "# 优化器\n",
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:    \n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sgd优化器数学原理：\n",
    "- 计算梯度：$\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w},b)} \\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{w}, b)$\n",
    "- 更新参数：$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\cdot \\mathbf{g}$\n",
    "- $\\eta$ 是学习率，控制更新步长\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01, correct rate on training set: 1.47%\n",
      "epoch 02, correct rate on training set: 12.61%\n",
      "epoch 03, correct rate on training set: 1.17%\n",
      "epoch 04, correct rate on training set: 21.99%\n",
      "epoch 05, correct rate on training set: 21.70%\n",
      "epoch 06, correct rate on training set: 21.99%\n",
      "epoch 07, correct rate on training set: 10.26%\n",
      "epoch 08, correct rate on training set: 1.17%\n",
      "epoch 09, correct rate on training set: 10.26%\n",
      "epoch 10, correct rate on training set: 21.99%\n",
      "epoch 11, correct rate on training set: 10.26%\n",
      "epoch 12, correct rate on training set: 12.02%\n",
      "epoch 13, correct rate on training set: 10.26%\n",
      "epoch 14, correct rate on training set: 1.17%\n",
      "epoch 15, correct rate on training set: 0.00%\n",
      "epoch 16, correct rate on training set: 9.97%\n"
     ]
    }
   ],
   "source": [
    "lr = 0.02\n",
    "num_epochs = 16\n",
    "net = linreg\n",
    "loss = mse_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, data_train, tags_train):\n",
    "        l = loss(net(X, w, b), y)  # X和y的小批量损失\n",
    "        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，\n",
    "        # 并以此计算关于[w,b]的梯度\n",
    "        l.sum().backward() # 求梯度\n",
    "        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n",
    "    with torch.no_grad():\n",
    "        train_l = torch.round(net(data_train, w, b)).reshape(tags_train.shape) == tags_train  # 训练集上的损失\n",
    "        print(f'epoch {epoch + 1:02d}, correct rate on training set: {train_l.sum()/len(data_train)*100:.02f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👆可以看到，正确率上不去，说明可能不存在线性可分的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct rate on test set: 6.98%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_l = torch.round(net(data_test, w, b)).reshape(tags_test.shape) == tags_test  # 训练集上的损失\n",
    "    print(f'correct rate on test set: {train_l.sum()/len(tags_test)*100:.02f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇方案2：softmax回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1, loss: 1.636, acc: 0.974\n",
      "epoch  2, loss: 1.229, acc: 0.974\n",
      "epoch  3, loss: 0.769, acc: 0.974\n",
      "epoch  4, loss: 0.691, acc: 0.974\n",
      "epoch  5, loss: 0.443, acc: 0.974\n",
      "epoch  6, loss: 0.343, acc: 0.974\n",
      "epoch  7, loss: 0.268, acc: 0.974\n",
      "epoch  8, loss: 0.253, acc: 0.974\n",
      "epoch  9, loss: 0.204, acc: 0.974\n",
      "epoch 10, loss: 0.359, acc: 0.974\n",
      "epoch 11, loss: 0.191, acc: 0.974\n",
      "epoch 12, loss: 0.152, acc: 0.974\n",
      "epoch 13, loss: 0.307, acc: 0.974\n",
      "epoch 14, loss: 0.140, acc: 0.974\n",
      "epoch 15, loss: 0.616, acc: 0.974\n",
      "epoch 16, loss: 0.259, acc: 0.974\n",
      "epoch 17, loss: 0.411, acc: 0.974\n",
      "epoch 18, loss: 0.241, acc: 0.974\n",
      "epoch 19, loss: 0.283, acc: 0.974\n",
      "epoch 20, loss: 0.097, acc: 0.974\n",
      "epoch 21, loss: 0.267, acc: 0.974\n",
      "epoch 22, loss: 0.234, acc: 0.974\n",
      "epoch 23, loss: 0.091, acc: 0.974\n",
      "epoch 24, loss: 0.256, acc: 0.974\n",
      "epoch 25, loss: 0.226, acc: 0.974\n",
      "epoch 26, loss: 0.081, acc: 0.974\n",
      "epoch 27, loss: 0.491, acc: 0.974\n",
      "epoch 28, loss: 0.220, acc: 0.974\n",
      "epoch 29, loss: 0.078, acc: 0.974\n",
      "epoch 30, loss: 0.067, acc: 0.974\n",
      "epoch 31, loss: 0.064, acc: 0.974\n",
      "epoch 32, loss: 0.319, acc: 0.974\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "def accuracy(y_hat, y):  \n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum()/len(y))\n",
    "\n",
    "\n",
    "# 定义数据集\n",
    "batch_size = 32\n",
    "net = nn.Sequential(nn.Flatten(), nn.Linear(384, 10))\n",
    "net.apply(init_weights)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.015)\n",
    "num_epochs = 32\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()  # 切换到训练模式\n",
    "    for X, y in data_iter(batch_size, data_train, tags_train):\n",
    "        # 计算梯度并更新参数\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "\n",
    "        # 使用PyTorch内置的优化器和损失函数\n",
    "        trainer.zero_grad()\n",
    "        l.mean().backward()\n",
    "        trainer.step()\n",
    "\n",
    "    # 计算训练准确度\n",
    "    with torch.no_grad():\n",
    "        acc = accuracy(net(data_train), tags_train)\n",
    "    print(f'epoch {epoch+1:2d}, loss: {l.mean().item():.3f}, acc: {acc:.3f}')\n",
    "    # net.eval()\n",
    "    # acc = accuracy(net(data_train), tags_train)\n",
    "    # print(f'epoch {epoch+1:2d}, loss: {l.mean().item():.3f}, acc: {acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.319, acc: 0.965\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat = net(data_test)\n",
    "    y = tags_test  # 标签转为LongTensor类型\n",
    "    acc = accuracy(y_hat, y)\n",
    "    print(f'loss: {l.mean().item():.3f}, acc: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇方案3：Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(384, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 10))\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights)\n",
    "batch_size, lr, num_epochs = 32, 0.1, 16\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.2860, train acc 0.814, test acc 0.256\n",
      "epoch 2, loss 0.2464, train acc 1.593, test acc 0.523\n",
      "epoch 3, loss 0.1567, train acc 2.977, test acc 0.837\n",
      "epoch 4, loss 0.0746, train acc 3.663, test acc 0.837\n",
      "epoch 5, loss 0.0421, train acc 3.837, test acc 0.965\n",
      "epoch 6, loss 0.0299, train acc 3.860, test acc 0.965\n",
      "epoch 7, loss 0.0235, train acc 3.860, test acc 0.965\n",
      "epoch 8, loss 0.0210, train acc 3.860, test acc 0.965\n",
      "epoch 9, loss 0.0191, train acc 3.860, test acc 0.965\n",
      "epoch 10, loss 0.0171, train acc 3.860, test acc 0.965\n",
      "epoch 11, loss 0.0183, train acc 3.860, test acc 0.965\n",
      "epoch 12, loss 0.0158, train acc 3.860, test acc 0.965\n",
      "epoch 13, loss 0.0160, train acc 3.860, test acc 0.965\n",
      "epoch 14, loss 0.0155, train acc 3.860, test acc 0.965\n",
      "epoch 15, loss 0.0148, train acc 3.860, test acc 0.965\n",
      "epoch 16, loss 0.0155, train acc 3.860, test acc 0.965\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "    for X, y in data_iter(batch_size, data_train, tags_train):\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        l = l.mean()\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_l_sum += l.item()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        n += y.shape[0]\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter(batch_size, data_test, tags_test):\n",
    "            y_hat = net(X)\n",
    "            acc_sum += (y_hat.argmax(dim=1) == y).float().sum().item()\n",
    "            n += y.shape[0]\n",
    "    # n = test_size\n",
    "    test_acc = acc_sum / n\n",
    "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "          % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇方案4：Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
