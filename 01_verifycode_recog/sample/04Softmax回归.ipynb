{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤”æ€è€ƒï¼šå¦‚ä½•åŠ è½½æ•°æ®ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels.txt\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "tags = []\n",
    "data = []\n",
    "for l in raw:\n",
    "    tags.append(int(l[0]))  # æ¯è¡Œçš„ç¬¬ä¸€ä¸ªå­—ç¬¦æ˜¯æ ‡ç­¾\n",
    "    d = l[1:-1]  # å»æ‰æ ‡ç­¾å’Œæ¢è¡Œç¬¦\n",
    "    d = map(float, tuple(d))  # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºtupleï¼Œæ•°å­—è½¬æ¢ä¸ºfloatï¼Œæ–¹ä¾¿åç»­è½¬ä¸ºtensor\n",
    "    # tupleç›¸å¯¹äºlistæ›´çœå†…å­˜ï¼Œå› ä¸ºtupleæ˜¯ä¸å¯å˜çš„ï¼Œå¯¹è±¡æ‰€å«methodæ›´å°‘\n",
    "    data.append(tuple(d))\n",
    "\n",
    "# å°†æ ‡ç­¾å’Œæ•°æ®è½¬ä¸ºtensorï¼Œæ–¹ä¾¿åç»­åˆ‡åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "data = torch.tensor(data)\n",
    "tags = torch.tensor(tags)\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_test_ratio = 0.8\n",
    "train_size = int(train_test_ratio * len(data))\n",
    "test_size = len(data) - train_size\n",
    "data_train = data[:train_size]\n",
    "data_test = data[train_size:]\n",
    "tags_train = tags[:train_size]\n",
    "tags_test = tags[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›´æ¥å¥—ç”¨d2lç½‘ç«™ä¸Šçš„ä»£ç ï¼Œæ²¡æœ‰æ”¹åŠ¨\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # è¿™äº›æ ·æœ¬æ˜¯éšæœºè¯»å–çš„ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡ºåº\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "w = torch.normal(0, 0.01, size=(\n",
    "    len(data[0]), 1), requires_grad=True, dtype=torch.float32)  # å¯¹æ¯ä¸ªåƒç´ éƒ½æœ‰ä¸€ä¸ªæƒé‡\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‡æ–¹æ¡ˆ2ï¼šsoftmaxå›å½’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ•°å­¦åŸç†ï¼š\n",
    "\n",
    "Softmaxå›å½’æ˜¯ä¸€ç§åˆ†ç±»ç®—æ³•ï¼Œå®ƒå¯ä»¥å°†è¾“å…¥æ•°æ®æ˜ å°„åˆ°ä¸€ä¸ªå›ºå®šç»´åº¦çš„è¾“å‡ºå‘é‡ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”äºè¾“å…¥æ•°æ®å±äºå„ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚\n",
    "\n",
    "- Softmaxå›å½’çš„å…¬å¼ï¼š\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y} &= \\text{softmax}(Wx+b) \\\\\n",
    "\n",
    "\\text{softmax}(x_i) &= \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "å…¶ä¸­ï¼Œ$W$å’Œ$b$æ˜¯æ¨¡å‹çš„å‚æ•°ï¼Œ$x$æ˜¯è¾“å…¥æ•°æ®ï¼Œ$\\hat{y}$æ˜¯æ¨¡å‹çš„è¾“å‡ºã€‚\n",
    "\n",
    "- Softmaxå›å½’çš„æŸå¤±å‡½æ•°ï¼š\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L &= -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^K t_{ij}\\log\\hat{y}_{ij} \\\\\n",
    "&= -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^K t_{ij}\\log\\frac{\\exp(x_i^T w_j + b_j)}{\\sum_{k=1}^K \\exp(x_i^T w_k + b_k)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼Œ$t_{ij}$æ˜¯æ ·æœ¬$i$çš„çœŸå®æ ‡ç­¾ï¼Œ$K$æ˜¯ç±»åˆ«æ•°ã€‚\n",
    "\n",
    "- Softmaxå›å½’çš„ä¼˜åŒ–ç®—æ³•ï¼š\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{repeat until convergence} \\\\\n",
    "\\text{for each example} \\\\\n",
    "&\\text{compute } \\hat{y}_i = \\text{softmax}(Wx_i+b) \\\\\n",
    "&\\text{compute } \\delta_i = \\hat{y}_i - t_i \\\\\n",
    "&\\text{compute } \\nabla_w L = \\frac{1}{N}\\sum_{i=1}^N \\delta_i x_i^T \\\\\n",
    "&\\text{compute } \\nabla_b L = \\frac{1}{N}\\sum_{i=1}^N \\delta_i \\\\\n",
    "&\\text{update } w = w - \\eta \\nabla_w L \\\\\n",
    "&\\text{update } b = b - \\eta \\nabla_b L\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- [Softmaxçš„å®ç°ï¼š](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-concise.html#subsec-softmax-implementation-revisited)\n",
    "\n",
    ">  å°½ç®¡æˆ‘ä»¬è¦è®¡ç®—æŒ‡æ•°å‡½æ•°ï¼Œä½†æˆ‘ä»¬æœ€ç»ˆåœ¨è®¡ç®—äº¤å‰ç†µæŸå¤±æ—¶ä¼šå–å®ƒä»¬çš„å¯¹æ•°ã€‚ é€šè¿‡å°†softmaxå’Œäº¤å‰ç†µç»“åˆåœ¨ä¸€\n",
    ">  èµ·ï¼Œå¯ä»¥é¿å…åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å¯èƒ½ä¼šå›°æ‰°æˆ‘ä»¬çš„æ•°å€¼ç¨³å®šæ€§é—®é¢˜ã€‚ å¦‚ä¸‹é¢çš„ç­‰å¼æ‰€ç¤ºï¼Œæˆ‘ä»¬\n",
    ">é¿å…è®¡ç®— $\\exp(o_j  - \\max(o_k))$ï¼Œ è€Œå¯ä»¥ç›´æ¥ä½¿ç”¨$o_j - \\max(o_k)$ï¼Œå› ä¸º$\\log(\\exp(\\cdot))$è¢«æŠµæ¶ˆäº†ã€‚\n",
    "\n",
    ">æˆ‘ä»¬ä¹Ÿå¸Œæœ›ä¿ç•™ä¼ ç»Ÿçš„softmaxå‡½æ•°ï¼Œä»¥å¤‡æˆ‘ä»¬éœ€è¦è¯„ä¼°é€šè¿‡æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡ã€‚ ä½†æ˜¯ï¼Œæˆ‘ä»¬æ²¡æœ‰å°†softmaxæ¦‚ç‡ä¼ \n",
    ">é€’åˆ°æŸå¤±å‡½æ•°ä¸­ï¼Œ è€Œæ˜¯åœ¨äº¤å‰ç†µæŸå¤±å‡½æ•°ä¸­ä¼ é€’æœªè§„èŒƒåŒ–çš„é¢„æµ‹ï¼Œå¹¶åŒæ—¶è®¡ç®—softmaxåŠå…¶å¯¹æ•°ï¼Œ è¿™æ˜¯ä¸€ç§ç±»ä¼¼\n",
    ">â€œLogSumExpæŠ€å·§â€çš„èªæ˜æ–¹å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1, loss: 1.614, acc: 0.894\n",
      "epoch  2, loss: 1.000, acc: 0.974\n",
      "epoch  3, loss: 0.848, acc: 0.974\n",
      "epoch  4, loss: 0.632, acc: 0.974\n",
      "epoch  5, loss: 0.413, acc: 0.974\n",
      "epoch  6, loss: 0.327, acc: 0.974\n",
      "epoch  7, loss: 0.426, acc: 0.974\n",
      "epoch  8, loss: 0.263, acc: 0.974\n",
      "epoch  9, loss: 0.389, acc: 0.974\n",
      "epoch 10, loss: 0.201, acc: 0.974\n",
      "epoch 11, loss: 0.177, acc: 0.974\n",
      "epoch 12, loss: 0.304, acc: 0.974\n",
      "epoch 13, loss: 0.153, acc: 0.974\n",
      "epoch 14, loss: 0.126, acc: 0.974\n",
      "epoch 15, loss: 0.286, acc: 0.974\n",
      "epoch 16, loss: 0.260, acc: 0.974\n",
      "epoch 17, loss: 0.260, acc: 0.974\n",
      "epoch 18, loss: 0.278, acc: 0.974\n",
      "epoch 19, loss: 0.102, acc: 0.974\n",
      "epoch 20, loss: 0.093, acc: 0.974\n",
      "epoch 21, loss: 0.263, acc: 0.974\n",
      "epoch 22, loss: 0.227, acc: 0.974\n",
      "epoch 23, loss: 0.419, acc: 0.974\n",
      "epoch 24, loss: 0.205, acc: 0.974\n",
      "epoch 25, loss: 0.210, acc: 0.974\n",
      "epoch 26, loss: 0.393, acc: 0.974\n",
      "epoch 27, loss: 0.090, acc: 0.974\n",
      "epoch 28, loss: 0.332, acc: 0.974\n",
      "epoch 29, loss: 0.665, acc: 0.974\n",
      "epoch 30, loss: 0.078, acc: 0.974\n",
      "epoch 31, loss: 0.077, acc: 0.974\n",
      "epoch 32, loss: 0.251, acc: 0.974\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"è®¡ç®—é¢„æµ‹æ­£ç¡®çš„æ•°é‡\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:  # å¤šåˆ†ç±»\n",
    "        y_hat = y_hat.argmax(axis=1)    # å–æœ€å¤§å€¼ä½œä¸ºé¢„æµ‹ç±»åˆ«\n",
    "    cmp = y_hat.type(y.dtype) == y  # é¢„æµ‹æ­£ç¡®çš„ä½ç½®ä¸ºTrue\n",
    "    return float(cmp.type(y.dtype).sum()/len(y))\n",
    "\n",
    "\n",
    "# å®šä¹‰æ•°æ®é›†\n",
    "batch_size = 32\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(384, 10)  # å®šä¹‰çº¿æ€§å±‚ï¼Œè¾“å…¥384ï¼Œè¾“å‡º10ä¸ªç±»åˆ«\n",
    ")\n",
    "net.apply(init_weights)  # åˆå§‹åŒ–æƒé‡\n",
    "loss = nn.CrossEntropyLoss(reduction='none')  # â€œLogSumExpæŠ€å·§â€\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.015)  # å®šä¹‰ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡\n",
    "num_epochs = 32  # è®­ç»ƒè½®æ•°\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼\n",
    "    for X, y in data_iter(batch_size, data_train, tags_train):\n",
    "        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "\n",
    "        # ä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°\n",
    "        trainer.zero_grad()\n",
    "        l.mean().backward() \n",
    "        trainer.step()  # æ›´æ–°å‚æ•°\n",
    "\n",
    "    # è®¡ç®—è®­ç»ƒå‡†ç¡®åº¦\n",
    "    net.eval()  # åˆ‡æ¢åˆ°æµ‹è¯•æ¨¡å¼\n",
    "    with torch.no_grad():  # è™½ç„¶net.eval()å¯ä»¥åˆ‡æ¢æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œä½†åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œè¿˜å»ºè®®ç»“åˆtorch.no_grad()ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¥è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚\n",
    "        acc = accuracy(net(data_train), tags_train)\n",
    "    print(f'epoch {epoch+1:2d}, loss: {l.mean().item():.3f}, acc: {acc:.3f}')\n",
    "    # net.eval()\n",
    "    # acc = accuracy(net(data_train), tags_train)\n",
    "    # print(f'epoch {epoch+1:2d}, loss: {l.mean().item():.3f}, acc: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.251, acc: 0.965\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat = net(data_test)\n",
    "    y = tags_test  # æ ‡ç­¾è½¬ä¸ºLongTensorç±»å‹\n",
    "    acc = accuracy(y_hat, y)\n",
    "    print(f'loss: {l.mean().item():.3f}, acc: {acc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
