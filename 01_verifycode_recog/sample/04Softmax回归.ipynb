{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤔思考：如何加载数据？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels.txt\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "tags = []\n",
    "data = []\n",
    "for l in raw:\n",
    "    tags.append(int(l[0]))  # 每行的第一个字符是标签\n",
    "    d = l[1:-1]  # 去掉标签和换行符\n",
    "    d = map(float, tuple(d))  # 将字符串转换为tuple，数字转换为float，方便后续转为tensor\n",
    "    # tuple相对于list更省内存，因为tuple是不可变的，对象所含method更少\n",
    "    data.append(tuple(d))\n",
    "\n",
    "# 将标签和数据转为tensor，方便后续切分训练集和测试集\n",
    "data = torch.tensor(data)\n",
    "tags = torch.tensor(tags)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_test_ratio = 0.8\n",
    "train_size = int(train_test_ratio * len(data))\n",
    "test_size = len(data) - train_size\n",
    "data_train = data[:train_size]\n",
    "data_test = data[train_size:]\n",
    "tags_train = tags[:train_size]\n",
    "tags_test = tags[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接套用d2l网站上的代码，没有改动\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "w = torch.normal(0, 0.01, size=(\n",
    "    len(data[0]), 1), requires_grad=True, dtype=torch.float32)  # 对每个像素都有一个权重\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇方案2：softmax回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学原理：\n",
    "\n",
    "Softmax回归是一种分类算法，它可以将输入数据映射到一个固定维度的输出向量，每个元素对应于输入数据属于各个类别的概率。\n",
    "\n",
    "- Softmax回归的公式：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y} &= \\text{softmax}(Wx+b) \\\\\n",
    "\n",
    "\\text{softmax}(x_i) &= \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "其中，$W$和$b$是模型的参数，$x$是输入数据，$\\hat{y}$是模型的输出。\n",
    "\n",
    "- Softmax回归的损失函数：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L &= -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^K t_{ij}\\log\\hat{y}_{ij} \\\\\n",
    "&= -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^K t_{ij}\\log\\frac{\\exp(x_i^T w_j + b_j)}{\\sum_{k=1}^K \\exp(x_i^T w_k + b_k)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中，$t_{ij}$是样本$i$的真实标签，$K$是类别数。\n",
    "\n",
    "- Softmax回归的优化算法：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{repeat until convergence} \\\\\n",
    "\\text{for each example} \\\\\n",
    "&\\text{compute } \\hat{y}_i = \\text{softmax}(Wx_i+b) \\\\\n",
    "&\\text{compute } \\delta_i = \\hat{y}_i - t_i \\\\\n",
    "&\\text{compute } \\nabla_w L = \\frac{1}{N}\\sum_{i=1}^N \\delta_i x_i^T \\\\\n",
    "&\\text{compute } \\nabla_b L = \\frac{1}{N}\\sum_{i=1}^N \\delta_i \\\\\n",
    "&\\text{update } w = w - \\eta \\nabla_w L \\\\\n",
    "&\\text{update } b = b - \\eta \\nabla_b L\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- [Softmax的实现：](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-concise.html#subsec-softmax-implementation-revisited)\n",
    "\n",
    ">  尽管我们要计算指数函数，但我们最终在计算交叉熵损失时会取它们的对数。 通过将softmax和交叉熵结合在一\n",
    ">  起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。 如下面的等式所示，我们\n",
    ">避免计算 $\\exp(o_j  - \\max(o_k))$， 而可以直接使用$o_j - \\max(o_k)$，因为$\\log(\\exp(\\cdot))$被抵消了。\n",
    "\n",
    ">我们也希望保留传统的softmax函数，以备我们需要评估通过模型输出的概率。 但是，我们没有将softmax概率传\n",
    ">递到损失函数中， 而是在交叉熵损失函数中传递未规范化的预测，并同时计算softmax及其对数， 这是一种类似\n",
    ">“LogSumExp技巧”的聪明方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1, loss: 1.614, acc: 0.894\n",
      "epoch  2, loss: 1.000, acc: 0.974\n",
      "epoch  3, loss: 0.848, acc: 0.974\n",
      "epoch  4, loss: 0.632, acc: 0.974\n",
      "epoch  5, loss: 0.413, acc: 0.974\n",
      "epoch  6, loss: 0.327, acc: 0.974\n",
      "epoch  7, loss: 0.426, acc: 0.974\n",
      "epoch  8, loss: 0.263, acc: 0.974\n",
      "epoch  9, loss: 0.389, acc: 0.974\n",
      "epoch 10, loss: 0.201, acc: 0.974\n",
      "epoch 11, loss: 0.177, acc: 0.974\n",
      "epoch 12, loss: 0.304, acc: 0.974\n",
      "epoch 13, loss: 0.153, acc: 0.974\n",
      "epoch 14, loss: 0.126, acc: 0.974\n",
      "epoch 15, loss: 0.286, acc: 0.974\n",
      "epoch 16, loss: 0.260, acc: 0.974\n",
      "epoch 17, loss: 0.260, acc: 0.974\n",
      "epoch 18, loss: 0.278, acc: 0.974\n",
      "epoch 19, loss: 0.102, acc: 0.974\n",
      "epoch 20, loss: 0.093, acc: 0.974\n",
      "epoch 21, loss: 0.263, acc: 0.974\n",
      "epoch 22, loss: 0.227, acc: 0.974\n",
      "epoch 23, loss: 0.419, acc: 0.974\n",
      "epoch 24, loss: 0.205, acc: 0.974\n",
      "epoch 25, loss: 0.210, acc: 0.974\n",
      "epoch 26, loss: 0.393, acc: 0.974\n",
      "epoch 27, loss: 0.090, acc: 0.974\n",
      "epoch 28, loss: 0.332, acc: 0.974\n",
      "epoch 29, loss: 0.665, acc: 0.974\n",
      "epoch 30, loss: 0.078, acc: 0.974\n",
      "epoch 31, loss: 0.077, acc: 0.974\n",
      "epoch 32, loss: 0.251, acc: 0.974\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:  # 多分类\n",
    "        y_hat = y_hat.argmax(axis=1)    # 取最大值作为预测类别\n",
    "    cmp = y_hat.type(y.dtype) == y  # 预测正确的位置为True\n",
    "    return float(cmp.type(y.dtype).sum()/len(y))\n",
    "\n",
    "\n",
    "# 定义数据集\n",
    "batch_size = 32\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(384, 10)  # 定义线性层，输入384，输出10个类别\n",
    ")\n",
    "net.apply(init_weights)  # 初始化权重\n",
    "loss = nn.CrossEntropyLoss(reduction='none')  # “LogSumExp技巧”\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.015)  # 定义优化器和学习率\n",
    "num_epochs = 32  # 训练轮数\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()  # 切换到训练模式\n",
    "    for X, y in data_iter(batch_size, data_train, tags_train):\n",
    "        # 计算梯度并更新参数\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "\n",
    "        # 使用PyTorch内置的优化器和损失函数\n",
    "        trainer.zero_grad()\n",
    "        l.mean().backward() \n",
    "        trainer.step()  # 更新参数\n",
    "\n",
    "    # 计算训练准确度\n",
    "    net.eval()  # 切换到测试模式\n",
    "    with torch.no_grad():  # 虽然net.eval()可以切换模型为评估模式，但在评估过程中，还建议结合torch.no_grad()上下文管理器来进一步优化性能。\n",
    "        acc = accuracy(net(data_train), tags_train)\n",
    "    print(f'epoch {epoch+1:2d}, loss: {l.mean().item():.3f}, acc: {acc:.3f}')\n",
    "    # net.eval()\n",
    "    # acc = accuracy(net(data_train), tags_train)\n",
    "    # print(f'epoch {epoch+1:2d}, loss: {l.mean().item():.3f}, acc: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.251, acc: 0.965\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat = net(data_test)\n",
    "    y = tags_test  # 标签转为LongTensor类型\n",
    "    acc = accuracy(y_hat, y)\n",
    "    print(f'loss: {l.mean().item():.3f}, acc: {acc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
