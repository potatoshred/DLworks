{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤”æ€è€ƒï¼šå¦‚ä½•åŠ è½½æ•°æ®ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\potatoshred\\AppData\\Local\\Temp\\ipykernel_25624\\2786306438.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:279.)\n",
      "  data = torch.tensor(data)\n"
     ]
    }
   ],
   "source": [
    "with open(\"labels.txt\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "tags = []\n",
    "data = []\n",
    "for l in raw:\n",
    "    tags.append(int(l[0]))#æ¯è¡Œçš„ç¬¬ä¸€ä¸ªå­—ç¬¦æ˜¯æ ‡ç­¾\n",
    "    d = l[1:-1]#å»æ‰æ ‡ç­¾å’Œæ¢è¡Œç¬¦\n",
    "    d = map(float, tuple(d)) #å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºtupleï¼Œæ•°å­—è½¬æ¢ä¸ºfloatï¼Œæ–¹ä¾¿åç»­è½¬ä¸ºtensor\n",
    "    #tupleç›¸å¯¹äºlistæ›´çœå†…å­˜ï¼Œå› ä¸ºtupleæ˜¯ä¸å¯å˜çš„ï¼Œå¯¹è±¡æ‰€å«methodæ›´å°‘\n",
    "    d = np.array(tuple(d)).reshape(24, 16).astype(np.float32) #å°†tupleè½¬æ¢ä¸º24x16çš„numpyæ•°ç»„ï¼Œå¹¶è½¬ä¸ºfloat32ç±»å‹\n",
    "    data.append(d)\n",
    "\n",
    "#å°†æ ‡ç­¾å’Œæ•°æ®è½¬ä¸ºtensorï¼Œæ–¹ä¾¿åç»­åˆ‡åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "data = torch.tensor(data)\n",
    "tags = torch.tensor(tags)\n",
    "\n",
    "#åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_test_ratio = 0.8\n",
    "train_size = int(train_test_ratio * len(data))\n",
    "test_size = len(data) - train_size\n",
    "data_train = data[:train_size]\n",
    "data_test  = data[train_size:]\n",
    "tags_train = tags[:train_size]\n",
    "tags_test  = tags[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›´æ¥å¥—ç”¨d2lç½‘ç«™ä¸Šçš„ä»£ç ï¼Œæ²¡æœ‰æ”¹åŠ¨\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # è¿™äº›æ ·æœ¬æ˜¯éšæœºè¯»å–çš„ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡ºåº\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "w = torch.normal(0, 0.01, size=(len(data[0]), 1), requires_grad=True, dtype=torch.float32) #å¯¹æ¯ä¸ªåƒç´ éƒ½æœ‰ä¸€ä¸ªæƒé‡\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‡æ–¹æ¡ˆ4ï¼šConvolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ•°å­¦åŸç†ï¼š\n",
    "\n",
    "å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvolutional Neural Networkï¼ŒCNNï¼‰æ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå®ƒæ˜¯ç”±å·ç§¯å±‚å’Œæ± åŒ–å±‚ç»„æˆçš„ã€‚\n",
    "\n",
    "- å·ç§¯å®šä¹‰ï¼š\n",
    "\n",
    "å·ç§¯ï¼Œæˆ–è¯‘ä¸ºå ç§¯ã€è¤¶ç§¯æˆ–æ—‹ç§¯ï¼Œæ˜¯é€è¿‡ä¸¤ä¸ªå‡½æ•°$f(x)$å’Œ$g(x)$çš„ä¹˜ç§¯ï¼Œå¾—åˆ°ç¬¬ä¸‰ä¸ªå‡½æ•°$h(x)$çš„ä¸€ç§è¿ç®—ã€‚\n",
    "å…¬å¼ï¼š\n",
    "\n",
    "$h(x) = (f*g)(x) = \\int_{-\\infty}^{\\infty} f(t)g(x-t)dt$\n",
    "\n",
    "\n",
    "![image.png](./img/conv1.gif)\n",
    "\n",
    "\n",
    "- å·ç§¯å±‚ï¼š\n",
    "\n",
    "å·ç§¯å±‚ç”±å¤šä¸ªå·ç§¯æ ¸ç»„æˆï¼Œæ¯ä¸ªå·ç§¯æ ¸ä¸è¾“å…¥å›¾åƒçš„å±€éƒ¨åŒºåŸŸè¿›è¡Œå·ç§¯æ“ä½œï¼Œæå–å›¾åƒçš„ç‰¹å¾ã€‚\n",
    "\n",
    "![image.png](./img/conv2.gif)\n",
    "\n",
    "å·ç§¯æ ¸å¯ä»¥çœ‹ä½œä¸€ç§æ•£åˆ—å‡½æ•°ï¼Œå®ƒå¯¹è¾“å…¥å›¾åƒçš„å±€éƒ¨åŒºåŸŸè¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå°†å¤šç»´çš„æ•°ç»„æ˜ å°„åˆ°ä¸€ç»´ã€‚ç„¶è€Œï¼Œä¸ä¼ ç»Ÿçš„æ•£åˆ—å‡½æ•°ä¸åŒï¼Œæ»‘åŠ¨çª—å£åœ¨è¿›è¡ŒåŠ æƒæ±‚å’Œæ—¶è€ƒè™‘äº†è¾“å…¥åŒºåŸŸçš„é‚»è¿‘å…³ç³»ï¼Œä»è€Œä¿ç•™äº†ä¸€å®šçš„å±€éƒ¨è¿ç»­æ€§ã€‚\n",
    "\n",
    "- æ± åŒ–/æ±‡èšå±‚ï¼šæ± åŒ–å±‚çš„ä½œç”¨æ˜¯é™ä½å·ç§¯å±‚å¯¹å›¾åƒçš„ç¼©æ”¾ç¨‹åº¦ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚\n",
    "  * æœ€å¤§æ± åŒ–ï¼šæœ€å¤§æ± åŒ–æ˜¯æŒ‡å¯¹å·ç§¯æ ¸è¦†ç›–çš„åŒºåŸŸå–æœ€å¤§å€¼ä½œä¸ºè¾“å‡ºã€‚æ„ä¹‰æ˜¯å‡å°‘ç‰¹å¾å›¾çš„å°ºå¯¸å¹¶æå–ä¸»è¦ç‰¹å¾ï¼Œå‡å°‘äº†å‚æ•°é‡ã€‚\n",
    "  + å¹³å‡æ± åŒ–ï¼šå¹³å‡æ± åŒ–æ˜¯æŒ‡å¯¹å·ç§¯æ ¸è¦†ç›–çš„åŒºåŸŸå–å¹³å‡å€¼ä½œä¸ºè¾“å‡ºã€‚ä¸æœ€å¤§æ± åŒ–ç›¸æ¯”ï¼Œå¹³å‡æ± åŒ–å¯ä»¥æ›´å¹³æ»‘åœ°ç¼©å‡ç‰¹å¾å›¾çš„å°ºå¯¸ï¼Œè€Œä¸å¤ªå…³æ³¨æœ€æ˜¾è‘—çš„ç‰¹å¾ã€‚æ›´åŠ å…³æ³¨å›¾åƒä¸­çš„æ•´ä½“åˆ†å¸ƒå’Œå¹³å‡å€¼ï¼Œè€Œä¸å¤ªå…³æ³¨å±€éƒ¨ç»†èŠ‚ã€‚é€‚ç”¨äºæå–å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„ä¸€äº›å…¨å±€ä¿¡æ¯ã€‚\n",
    "\n",
    "- å…¨è¿æ¥å±‚ï¼š\n",
    "\n",
    "å…¨è¿æ¥å±‚çš„ä½œç”¨æ˜¯å°†å·ç§¯å±‚æå–çš„ç‰¹å¾è¿æ¥åˆ°è¾“å‡ºå±‚ï¼Œè¾“å‡ºå±‚çš„æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”äºè¾“å…¥å›¾åƒçš„æŸä¸ªç‰¹å¾ã€‚\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n",
      "epoch 1, loss 0.0787, train acc 0.106, test acc 0.130\n",
      "epoch 2, loss 0.0721, train acc 0.158, test acc 0.130\n",
      "epoch 3, loss 0.0714, train acc 0.170, test acc 0.315\n",
      "epoch 4, loss 0.0429, train acc 0.566, test acc 0.963\n",
      "epoch 5, loss 0.0094, train acc 0.953, test acc 0.981\n",
      "epoch 6, loss 0.0076, train acc 0.974, test acc 0.944\n",
      "epoch 7, loss 0.0048, train acc 0.977, test acc 0.963\n",
      "epoch 8, loss 0.0055, train acc 0.974, test acc 0.963\n",
      "epoch 9, loss 0.0034, train acc 0.974, test acc 0.963\n",
      "epoch 10, loss 0.0026, train acc 0.977, test acc 0.963\n",
      "epoch 11, loss 0.0033, train acc 0.974, test acc 0.963\n",
      "epoch 12, loss 0.0023, train acc 0.982, test acc 0.963\n",
      "epoch 13, loss 0.0027, train acc 0.979, test acc 0.944\n",
      "epoch 14, loss 0.0022, train acc 0.977, test acc 0.944\n",
      "epoch 15, loss 0.0029, train acc 0.971, test acc 0.963\n",
      "epoch 16, loss 0.0033, train acc 0.979, test acc 0.944\n"
     ]
    }
   ],
   "source": [
    "# å®šä¹‰CNNæ¨¡å‹ï¼Œ ç›´æ¥ä½¿ç”¨pytorchçš„API\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # å·ç§¯å±‚1ï¼Œå¡«å……1ï¼Œ å®½åº¦å¢åŠ 2ï¼Œå·ç§¯æ ¸å¤§å°3x3ï¼Œè¾“å‡ºé•¿å®½ä»ä¸º16*24\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1,padding=1) \n",
    "        self.relu = nn.ReLU()  # æ¿€æ´»å‡½æ•°\n",
    "        # æ± åŒ–å±‚1ï¼Œ é•¿å®½å‡ç¼©å‡ä¸€åŠï¼Œ è¾“å‡ºé•¿å®½ä¸º8*12\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "        # # å·ç§¯å±‚2ï¼Œå·ç§¯æ ¸å¤§å°æ˜¯3x3ï¼Œè¾“å‡ºé•¿å®½ä¸º6*10\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1) \n",
    "        # 2æ¬¡å·ç§¯+æ± åŒ–åï¼Œå½“å‰å›¾ç‰‡çš„å¤§å°æ˜¯3x5ï¼Œ 64ä¸ªé€šé“ã€‚\n",
    "        self.fc1 = nn.Linear(32*3*5, 128)  # å…¨è¿æ¥å±‚1, è¾“å‡º128ä¸ªç‰¹å¾\n",
    "        self.fc2 = nn.Linear(128, 10)  # å…¨è¿æ¥å±‚2, è¾“å‡º10ä¸ªåˆ†ç±»\n",
    "\n",
    "    def forward(self, x):\n",
    "        # å·ç§¯å±‚1ï¼Œå¡«å……1ï¼Œ å®½åº¦å¢åŠ 2ï¼Œå·ç§¯æ ¸å¤§å°3x3ï¼Œè¾“å‡ºé•¿å®½ä»ä¸º16*24\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        # æ± åŒ–å±‚1ï¼Œ é•¿å®½å‡ç¼©å‡ä¸€åŠï¼Œ è¾“å‡ºé•¿å®½ä¸º8*12\n",
    "        x = self.maxpool(x)\n",
    "        # å·ç§¯å±‚2ï¼Œå·ç§¯æ ¸å¤§å°æ˜¯3x3ï¼Œè¾“å‡ºé•¿å®½ä¸º6*10\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        # æ± åŒ–å±‚2ï¼Œ é•¿å®½å‡ç¼©å‡ä¸€åŠï¼Œ è¾“å‡ºé•¿å®½ä¸º3*5\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # å…¨è¿æ¥å±‚1, è¾“å‡º128ä¸ªç‰¹å¾\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        # å…¨è¿æ¥å±‚2, è¾“å‡º10ä¸ªåˆ†ç±»\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# å®šä¹‰è¯„ä¼°å‡½æ•°\n",
    "def evaluate_accuracy(data_iter, net, device):\n",
    "    if not any(True for _ in data_iter):  # æ£€æŸ¥data_iteræ˜¯å¦ä¸ºç©º\n",
    "        return 0.0\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            net.eval()\n",
    "            X = X.reshape((-1, 1, 24, 16)).to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            net.train()\n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "# å®šä¹‰è®­ç»ƒå‡½æ•°\n",
    "def train_cnn(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
    "    net = net.to(device)  # å°†æ¨¡å‹ç§»åˆ°CPUæˆ–GPUä¸Šï¼ŒGPUåŠ é€Ÿè®¡ç®—\n",
    "    print(\"training on\", device)\n",
    "    loss = nn.CrossEntropyLoss() \n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in data_iter(batch_size, data_train, tags_train):\n",
    "            X = X.reshape((-1, 1, 24, 16)).to(device)  # æ”¹å˜å½¢çŠ¶å¹¶ç§»åˆ°device\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(data_iter(batch_size, data_test, tags_test), net, device)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "\n",
    "# å®šä¹‰è®­ç»ƒå‚æ•°\n",
    "lr, num_epochs = 0.02, 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # é€‰æ‹©CPUæˆ–GPU\n",
    "net = CNN()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)  # ç›¸æ¯”äºSGD, Adamä¼˜åŒ–å™¨å¯ä»¥è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡\n",
    "train_cnn(net, data_iter(batch_size, data_train, tags_train), data_iter(batch_size, data_test, tags_test), batch_size, optimizer, device, num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®­ç»ƒæ¨¡å‹ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "torch.save(net.state_dict(), 'cnn_model.pth')\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "net = CNN()\n",
    "net.load_state_dict(torch.load('cnn_model.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
